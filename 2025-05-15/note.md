# CMU 15-418 Lecture 1 学习笔记
## 主题：Why Parallelism? Why Efficiency?

---

## 一、并行计算的动机

### 1.1 单核性能发展受限

- 早期处理器依赖**提高时钟频率**来提升性能，但随着频率上升导致**功耗与热量过高**，这一策略在2000年代失效。
- **Dennard 缩放失效**：晶体管缩小的同时功耗不再按比例降低，频率停滞。
- **单线程性能增长放缓**，硬件厂商开始采用**多核架构**，用多个处理核心**并行执行多个指令**。

### 1.2 并行已成计算常态

- 现代计算设备（服务器、手机、手表）普遍配备多个处理核心。
- **并行计算已成为现代计算的标配**。
- 应对 AI、图形处理、科学仿真等计算密集型任务，**并行是必需的**。

---

## 二、为何强调效率？

### 2.1 并行≠自动提速

- 并行计算**可以显著提升程序运行速度**，但前提是程序必须**显式进行并行编程**。
- 仅仅“有多核”并不意味着“程序就快”。

### 2.2 资源不是免费的

- 多核芯片带来更多功耗、热量、芯片面积的代价。
- 若不能**高效利用资源**，反而带来浪费和性能瓶颈。

### 2.3 效率衡量指标

- **延迟（Latency）**：完成一个任务的时间。
- **吞吐量（Throughput）**：单位时间完成任务数。
- **资源利用率**：计算资源实际使用与理论最大值的比值。
- **能效比（Performance per Watt）**也是关键设计目标。

>  **效率是核心指标**：衡量的不只是性能本身，还有面积、能耗、可扩展性。

---

## 三、并行计算的核心挑战

### 3.1 通信延迟和同步开销

- 并行任务间需要共享或传递数据，引入**通信开销**。
- **通信是限制性能的关键因素**，通信量越大，程序越慢。
- 同步操作（锁、屏障等）导致线程等待，进一步降低效率。

### 3.2 负载不平衡

- **负载不平衡是常见问题**：若某些线程工作量较大，其他线程空闲，系统整体效率下降。
- 良好的负载均衡策略是并行程序设计的关键之一。

### 3.3 可扩展性与瓶颈

- 并非加更多核就一定更快。
- **Amdahl 定律**限制加速比：串行部分成为瓶颈。
- 程序扩展性越差，核数越多的价值就越小。

---

## 四、并行计算设计原则

### 4.1 结合硬件理解编程

- **并行程序必须理解底层硬件**的运行机制，包括缓存一致性、内存层级、线程调度等。
- 忽视硬件特性会导致程序无法利用性能潜力。

### 4.2 软硬件协同优化

- 优化并行程序不仅是写代码，更是**对计算架构、通信模型、任务分配**的深度设计。
- 性能、面积与能耗之间需进行平衡。

---

## 五、关键性能法则

| 定律名称       | 含义                                                         |
|----------------|--------------------------------------------------------------|
| **Amdahl's Law**     | 加速比受限于串行部分：`Speedup ≤ 1 / (s + (1 - s)/P)`      |
| **Gustafson's Law**  | 通过增加问题规模，提升核利用率，提高加速比                 |
| **Moore's Law**      | 每 18-24 个月晶体管数量翻倍，硬件容量指数增长              |
| **Dennard Scaling**  | 理论上缩小晶体管应减少功耗（已失效）                       |

---

## 六、并行计算的未来

- 并行计算是不可逆转的发展趋势。
- 掌握并行编程技能，将成为未来工程师的**核心竞争力**。
- 本课程将教你如何**在复杂系统中高效利用并行计算资源**，设计具有可扩展性的高性能程序。

---

## 七、术语整理

- **Latency（延迟）**：任务完成所需时间。
- **Throughput（吞吐量）**：单位时间处理任务量。
- **Load Balance（负载均衡）**：工作在各核间均匀分配。
- **Synchronization（同步）**：线程协调机制，会引入等待。
- **Communication Overhead（通信开销）**：线程间数据交换带来的时间成本。
- **Scalability（可扩展性）**：系统随核心数增加能否继续提升性能。

---

>  **思考**：
> 并行计算的本质不是“把任务分给多个核”，而是要**让多个核协同工作、避免互相阻碍、最大限度提高计算效率**。它是架构、算法、系统之间的协调。

